Project Overview:
	The goal of this project was to gather data from ratemyprofessors.com and do analysis to find similarities and differences in how students rate male versus female faculty. I wanted to use both sentiment analysis as well as word frequency to understand gendered language. The sentiment analysis was to understand how positive/negative students were as well as subjective (on average for male/female faculty). The word frequency analysis was to better understand gendered language and how different adjectives (such as 'funny', 'smart', mean), change with gender (there has been a lot of research to say that student's exception of their teacher changes with their gender). 

Implementation:
	Basically, my code consists of 4 functions: the first gets all the data, the second two do sentiment analysis and word frequency analysis (respectively), and the last does the analysis on the output of the data. The data output of the first function is two dictionaries. The first is of all the female faculty members and their respective comments on ratemyprofessor.com and the second is of all the male faculty members and their respective comments on ratemyprofessor.com. These four functions work together to come up with four different dictionaries: one with male names and their respective sentiment analysis of their comments, the same for the female faculty, and then two dictionaries that related gender to the frequency of adjectives (which can be changed). The plot.py does all of the data anlysis of the dictionaries. It averages the sentiment analysis and normalizes the usage of adjectives (so that the number is not skewed by the number of professors).
	Because ratemyprofessor.com doesn't have the gender of all their professors on the site, I had to make a design decision as to how to get this information. After doing some research I decided to use an external source code called Gender Predictor, which used machine learning from US census data to predict whether or not a name was male of female. Granted this didn't work every time, but it was simple to impliment and would also give a score as to how confident the machine learning was in their prediction as well as would not count "gender-neutral" words. 
	Another design decision that I had to make was when I was trying to find the urls for pattern to download. I decided to just iterate through a number of professors and if that number did not have a url associated with it, it would skip and move to the next part of the loop. Even though this would not give me the same number of professors as the n that I inputted, it was much more simple than trying to make sure that the download of the url gave me accurate information. Also, because I had plans to use it for a large n value, the actual number of professors didn't matter as much. 

Results:
	First off, looking at the sentiment analysis, I thought it was interesting just how many more male professors there are than female professors (I ended up looking at 1464 male faculty and 720 female faculty).Before I normalized the data, I was a little confused as to why there were sometimes double, even triple times the instances of certain words. I knew that there was a discrepancy between gender in professors, but seeing it in the graph showed just how large that difference is. 
	Once I normalized all the data, one thing that I found interesting was that there was very little difference in the sentiment analysis between gender. Both men and women had a positivity rating of 0.6 (which is on the more positive side) and both had a subjectivity rating of 0.18 (which is on the non-subjective side). I wasn't that surprized by the positivity rating (it seems that people would be realtively nice to their professors), but the subjectivity rating was not what I was expecting. Anectdotally, I found that when my peers are talking about their professors they aren't the most objective and (as with talking about any person), their emotions played a big part. I would want to do some more research to see exactly how they came up with this subjectivity rating because there seems to be a gap between what I expected and what I got. 
	The graph of the word analysis can be found here: https://plot.ly/~aditijoshi/80/adjectives-used-to-describe-professors-on-ratemyprofessorcom/, which I created using plot.ly. This was super interesting because when normalized, you can see that 'easy' and 'hard' are the most often used (and 'easy' is used more for women and 'hard' more for men). Also women are considered more 'nice' and men more 'funny'. Another interesting thing is that there are certain words that have a frequency of greater than 1, which means that the word was used multiple times for the same professor. This project taught me a lot and I think I got a lot of interesting data. 

Reflection:  
	I learned a lot from this project. I think from a process point of view, writing the functions bit by bit were extremely helpful. If I were to do it again, I would break up the first function into many different functions because that would let me better use doctests to make sure the smaller chunks were working. Doctest were rather difficult in this project because I had to use length of text often to make sure that I was getting the right results because the actual text would keep changing. Also, because of my earlier design decision (of using pass if the url did not exist), I could not accurately find the length of the dictionaries because sometimes the urls would not exist so I could not predict the length. I've also learned a lot by using external code and libraries (such as genderPredictor and plotly) which has been great. Also, being able to connect and download html from urls has been a great experience and it has connected a lot of the work to the work other people have been doing (in the "real world").